{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701b853f",
   "metadata": {},
   "source": [
    "<h1> Customer Experience Analytics for Fintech Apps </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a57a212",
   "metadata": {},
   "source": [
    "A Real-World Data Engineering Challenge: Scraping, Analyzing, and Visualizing Google Play Store Reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c228c3d",
   "metadata": {},
   "source": [
    "Task-1: Data Collection and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895585c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "from google_play_scraper import Sort, reviews\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import schedule\n",
    "import logging\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e54a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(filename='scraper.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37040d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define app IDs and their corresponding bank names\n",
    "APP_DETAILS = {\n",
    "    'com.commercialbankofethiopia.mobilebanking': 'Commercial Bank of Ethiopia',\n",
    "    'com.bankofabyssinia.mobilebanking': 'Bank of Abyssinia',\n",
    "    'com.dashen.dashensuperapp': 'Dashen Bank'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b2028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_play_store_reviews():\n",
    "    all_reviews_data = []\n",
    "\n",
    "    for app_id, bank_name in APP_DETAILS.items():\n",
    "        logging.info(f\"ðŸ”„ Fetching reviews for {bank_name} (App ID: {app_id})...\")\n",
    "        try:\n",
    "            results, _ = reviews(\n",
    "                app_id,\n",
    "                lang='en',\n",
    "                country='us',\n",
    "                sort=Sort.NEWEST,\n",
    "                count=4000,  # Increased count to ensure 400+ unique reviews per bank\n",
    "                filter_score_with=None\n",
    "            )\n",
    "\n",
    "            for entry in results:\n",
    "                all_reviews_data.append({\n",
    "                    'review_text': entry['content'],\n",
    "                    'rating': entry['score'],\n",
    "                    'date': entry['at'].strftime('%Y-%m-%d'),\n",
    "                    'bank': bank_name,\n",
    "                    'source': 'Google Play'\n",
    "                })\n",
    "            logging.info(f\"âœ… Fetched {len(results)} reviews for {bank_name}.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error occurred while scraping {bank_name} (App ID: {app_id}): {e}\")\n",
    "\n",
    "    if not all_reviews_data:\n",
    "        logging.warning(\"No reviews were scraped. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Convert to DataFrame for easier preprocessing\n",
    "    df = pd.DataFrame(all_reviews_data)\n",
    "\n",
    "    # Preprocessing\n",
    "    # Remove duplicates\n",
    "    initial_rows = len(df)\n",
    "    df.drop_duplicates(subset=['review_text', 'bank', 'date'], inplace=True)\n",
    "    logging.info(f\"Removed {initial_rows - len(df)} duplicate reviews.\")\n",
    "\n",
    "    # Handle missing data (e.g., drop rows where review_text or rating is missing)\n",
    "    df.dropna(subset=['review_text', 'rating'], inplace=True)\n",
    "    logging.info(f\"Remaining reviews after dropping NaNs: {len(df)}\")\n",
    "\n",
    "    # Normalize dates (already handled during scraping with strftime('%Y-%m-%d'))\n",
    "    # Ensure date column is in datetime format for consistency, though string format is fine for CSV\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "    # Save as CSV\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'all_banks_reviews_{timestamp}.csv'\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    logging.info(f\"âœ… Saved {len(df)} unique and preprocessed reviews to {filename}\")\n",
    "\n",
    "# Different scheduling options (uncomment the one you want to use):\n",
    "# schedule.every().day.at(\"01:00\").do(scrape_play_store_reviews)  # Daily at 1 AM\n",
    "# schedule.every(6).hours.do(scrape_play_store_reviews)           # Every 6 hours\n",
    "# schedule.every().monday.do(scrape_play_store_reviews)           # Every Monday\n",
    "schedule.every(1).minute.do(scrape_play_store_reviews)             # Every minute for testing\n",
    "\n",
    "# To run immediately for testing without waiting for schedule\n",
    "# scrape_play_store_reviews()\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
