{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701b853f",
   "metadata": {},
   "source": [
    "<h1> Customer Experience Analytics for Fintech Apps </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a57a212",
   "metadata": {},
   "source": [
    "A Real-World Data Engineering Challenge: Scraping, Analyzing, and Visualizing Google Play Store Reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c228c3d",
   "metadata": {},
   "source": [
    "Task-1: Data Collection and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "895585c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "from google_play_scraper import Sort, reviews\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import schedule\n",
    "import logging\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e939a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\python\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (2.2.2)\n",
      "Collecting schedule\n",
      "  Downloading schedule-1.2.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\python\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\python\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\python\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\python\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading schedule-1.2.2-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: schedule\n",
      "Successfully installed schedule-1.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e54a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(filename='scraper.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37040d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define app IDs and their corresponding bank names\n",
    "APP_DETAILS = {\n",
    "    'com.commercialbankofethiopia.mobilebanking': 'Commercial Bank of Ethiopia',\n",
    "    'com.bankofabyssinia.mobilebanking': 'Bank of Abyssinia',\n",
    "    'com.dashen.dashensuperapp': 'Dashen Bank'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b2028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_play_store_reviews():\n",
    "    all_reviews_data = []\n",
    "\n",
    "    for app_id, bank_name in APP_DETAILS.items():\n",
    "        logging.info(f\"ðŸ”„ Fetching reviews for {bank_name} (App ID: {app_id})...\")\n",
    "        try:\n",
    "            results, _ = reviews(\n",
    "                app_id,\n",
    "                lang='en',\n",
    "                country='us',\n",
    "                sort=Sort.NEWEST,\n",
    "                count=4000,  # Increased count to ensure 400+ unique reviews per bank\n",
    "                filter_score_with=None\n",
    "            )\n",
    "\n",
    "            for entry in results:\n",
    "                all_reviews_data.append({\n",
    "                    'review_text': entry['content'],\n",
    "                    'rating': entry['score'],\n",
    "                    'date': entry['at'].strftime('%Y-%m-%d'),\n",
    "                    'bank': bank_name,\n",
    "                    'source': 'Google Play'\n",
    "                })\n",
    "            logging.info(f\"âœ… Fetched {len(results)} reviews for {bank_name}.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error occurred while scraping {bank_name} (App ID: {app_id}): {e}\")\n",
    "\n",
    "    if not all_reviews_data:\n",
    "        logging.warning(\"No reviews were scraped. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Convert to DataFrame for easier preprocessing\n",
    "    df = pd.DataFrame(all_reviews_data)\n",
    "\n",
    "    # Preprocessing\n",
    "    # Remove duplicates\n",
    "    initial_rows = len(df)\n",
    "    df.drop_duplicates(subset=['review_text', 'bank', 'date'], inplace=True)\n",
    "    logging.info(f\"Removed {initial_rows - len(df)} duplicate reviews.\")\n",
    "\n",
    "    # Handle missing data (e.g., drop rows where review_text or rating is missing)\n",
    "    df.dropna(subset=['review_text', 'rating'], inplace=True)\n",
    "    logging.info(f\"Remaining reviews after dropping NaNs: {len(df)}\")\n",
    "\n",
    "    # Normalize dates (already handled during scraping with strftime('%Y-%m-%d'))\n",
    "    # Ensure date column is in datetime format for consistency, though string format is fine for CSV\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "    # Save as CSV\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = f'data/all_banks_reviews_{timestamp}.csv'\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    logging.info(f\"âœ… Saved {len(df)} unique and preprocessed reviews to {filename}\")\n",
    "\n",
    "# Different scheduling options (uncomment the one you want to use):\n",
    "# schedule.every().day.at(\"01:00\").do(scrape_play_store_reviews)  # Daily at 1 AM\n",
    "# schedule.every(6).hours.do(scrape_play_store_reviews)           # Every 6 hours\n",
    "# schedule.every().monday.do(scrape_play_store_reviews)           # Every Monday\n",
    "schedule.every(1).minute.do(scrape_play_store_reviews)             # Every minute for testing\n",
    "\n",
    "# To run immediately for testing without waiting for schedule\n",
    "# scrape_play_store_reviews()\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
